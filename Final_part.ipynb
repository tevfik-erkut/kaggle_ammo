{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\ndf = pd.read_csv(\"excluded_df.csv\")\ndf2 = pd.read_csv(\"included_df.csv\")\n\ndf=  df[['BASE_CUSTOMER_ID', 'req_brand_code_ratio',\"REQ_TOPMODEL_CODE\"]]\ndf2.drop([ 'req_brand_code_ratio','req_topmodel_code_ratio'], 1, inplace = True)\n\ndf_union = pd.merge(df2, df, on = \"BASE_CUSTOMER_ID\")\ndf_union.drop([\"Age\"], 1, inplace = True)\n\n\ndef serial_date_to_string(srl_no):\n    new_date = datetime.datetime(2021,5,31,0,0) - datetime.timedelta(srl_no)\n    return new_date.strftime(\"%Y-%m-%d\")\ntest_df = pd.read_csv(\"included_df.csv\")\nold_df = pd.read_csv(\"excluded_df.csv\")\nold_df=  old_df[['BASE_CUSTOMER_ID', 'req_brand_code_ratio',\"REQ_TOPMODEL_CODE\"]]\ntest_df.drop(['req_brand_code_ratio','req_topmodel_code_ratio'], 1, inplace = True)\n\ntest_df = pd.merge(test_df, old_df, on = \"BASE_CUSTOMER_ID\")\ntest_df.drop([\"Age\"],1 ,inplace = True)\ndf_union.drop([\"SALESFILE_ID\", \"SF_CREATE_DATE\", \"VEHICLE_ID\"], 1,inplace = True)\ndf_union.drop([\"FK_RELATION_STATUS_ID\"],1 , inplace = True)\ntest_df.drop([\"SALESFILE_ID\", \"SF_CREATE_DATE\", \"VEHICLE_ID\"], 1,inplace = True)\ntest_df.drop([\"FK_RELATION_STATUS_ID\"],1 , inplace = True)\n\nnew_idea = pd.read_csv(\"repair_extr_features_excluded.csv\")\nnew_idea_test = pd.read_csv(\"repair_extr_features_included.csv\")\ndf_union = df_union.merge(new_idea, on = \"BASE_CUSTOMER_ID\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create(hyperparams):\n    \"\"\"Create LGBM Classifier for a given set of hyper-parameters.\"\"\"\n    model = LGBMClassifier(**hyperparams)\n    return model\n\ndef fit(model, X, y):\n    \"\"\"Simple training of a given model.\"\"\"\n    model.fit(X, y)\n    return model\n\ndef fit_with_stop(model, X, y, X_val, y_val, esr):\n    \"\"\"Advanced training with early stopping.\"\"\"\n    model.fit(X, y,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=esr, \n              verbose=200)\n    return model\n\ndef evaluate(model, X, y):\n    \"\"\"Compute AUC for a given model.\"\"\"\n    yp = model.predict_proba(X)[:, 1]\n    auc_score = roc_auc_score(y, yp)\n    return auc_score\n\ndef kfold_evaluation(X, y, k, hyperparams, esr=100):\n    \"\"\"Run a KFlod evaluation.\"\"\"\n    scores = []\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        train_score = evaluate(model, X_train, y_train)\n        val_score = evaluate(model, X_val, y_val)\n        scores.append((train_score, val_score))\n        \n        print(f\"Fold {i} | Eval AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns=['train score', 'validation score'])\n    \n    return scores\n\ndef kfold_prediction(X, y, X_test, k, hyperparams, esr=100):\n    \"\"\"Make predictions with a bagged model based on KFold.\"\"\"\n    yp = np.zeros(len(X_test))\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        yp += model.predict_proba(X_test)[:, 1] / k\n    \n    return yp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant\nK = 10\nX_TEST =  test_df.drop([\"BASE_CUSTOMER_ID\",\"target\"], 1)\nBEST_PARAMS = {'n_estimators': 10000, 'learning_rate': 0.05,\n 'metric': 'auc'\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objective function\ndef objective(trial):\n    # Search spaces\n    hyperparams = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 5, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 64),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        \"is_unbalance\" : True\n    }\n    \n    # Add BEST_PARAMS\n    hyperparams.update(BEST_PARAMS)\n    \n    # Evaluation\n    scores = kfold_evaluation(X, Y, K, hyperparams, 100)\n    \n    return scores['validation score'].mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_union.drop([\"BASE_CUSTOMER_ID\", \"target\"], 1)\nY = df_union.target\ntarget=  \"target\"\ndef create(hyperparams):\n    \"\"\"Create LGBM Classifier for a given set of hyper-parameters.\"\"\"\n    model = LGBMClassifier(**hyperparams)\n    return model\n\ndef fit(model, X, y):\n    \"\"\"Simple training of a given model.\"\"\"\n    model.fit(X, y)\n    return model\n\ndef fit_with_stop(model, X, y, X_val, y_val, esr):\n    \"\"\"Advanced training with early stopping.\"\"\"\n    model.fit(X, y,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=esr, \n              verbose=200)\n    return model\n\ndef evaluate(model, X, y):\n    \"\"\"Compute AUC for a given model.\"\"\"\n    yp = model.predict_proba(X)[:, 1]\n    auc_score = roc_auc_score(y, yp)\n    return auc_score\n\ndef kfold_evaluation(X, y, k, hyperparams, esr=100):\n    \"\"\"Run a KFlod evaluation.\"\"\"\n    scores = []\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        train_score = evaluate(model, X_train, y_train)\n        val_score = evaluate(model, X_val, y_val)\n        scores.append((train_score, val_score))\n        \n        print(f\"Fold {i} | Eval AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns=['train score', 'validation score'])\n    \n    return scores\n\ndef kfold_prediction(X, y, X_test, k, hyperparams, esr=100):\n    \"\"\"Make predictions with a bagged model based on KFold.\"\"\"\n    yp = np.zeros(len(X_test))\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        yp += model.predict_proba(X_test)[:, 1] / k\n    \n    return yp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, timeout=3600*1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BEST_PARAMS.update(study.BEST_PARAMS)\n#BEST_PARAMS = {'reg_alpha': 2.1557466386247137, 'reg_lambda': 3.859970718309284, 'num_leaves': 343, \n #'min_child_samples': 32, 'max_depth': 7, 'colsample_bytree': 0.2067123441854414, 'cat_smooth': 35, \n  #'cat_l2': 16, 'min_data_per_group': 90,'learning_rate': 0.005, 'metric': 'auc'}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"sample_submission-Copy1.csv\")\ntest.rename(columns = {\"Id\": \"BASE_CUSTOMER_ID\"}, inplace = True)\ntest = pd.merge(test, test_df, on = \"BASE_CUSTOMER_ID\", how = \"left\")\ntest[\"target\"] = kfold_prediction(X, Y, test.drop([\"BASE_CUSTOMER_ID\", \"target\",\"Expected\"] ,1), 10, BEST_PARAMS, 500)\nsubmission = pd.read_csv(\"sample_submission-Copy1.csv\")\nsubmission.Expected = test.target.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission1.csv\", index = False)","metadata":{},"execution_count":null,"outputs":[]}]}